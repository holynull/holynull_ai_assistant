{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import   List\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import Tool\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "import re\n",
    "from html import unescape\n",
    "from openai_assistant_tools import TradingviewWrapper\n",
    "from langchain.agents import tool\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from openai_assistant_tools import GoogleSerperAPIWrapper\n",
    "from openai_assistant_tools import MyAPIChain\n",
    "\n",
    "import openai_assistant_api_docs\n",
    "\n",
    "import openai_assistant_api_docs\n",
    "from langchain.agents import  AgentExecutor\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import asyncio\n",
    "\n",
    "# from langchain.agents.output_parsers import JSONAgentOutputParser\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "\n",
    "\n",
    "def getTools()->:\n",
    "# agent_cb_manager = AsyncCallbackManager([agent_cb_handler])\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        verbose=True,\n",
    "    )\n",
    "    headers = {\n",
    "        \"Accepts\": \"application/json\",\n",
    "        \"X-CMC_PRO_API_KEY\": os.getenv(\"CMC_API_KEY\"),\n",
    "    }\n",
    "    cmc_last_quote_api = MyAPIChain.from_llm_and_api_docs(\n",
    "        llm=llm,\n",
    "        api_docs=openai_assistant_api_docs.cmc_quote_lastest_api_doc,\n",
    "        headers=headers,\n",
    "        limit_to_domains=[\"https://pro-api.coinmarketcap.com\"],\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    @tool\n",
    "    def getTokenMetadata(symbol: str) -> str:\n",
    "        \"\"\"\n",
    "        Useful when you need get the metadata of a token.\n",
    "        \"\"\"\n",
    "        url = (\n",
    "            f\"https://pro-api.coinmarketcap.com/v2/cryptocurrency/info?symbol={symbol}\"\n",
    "        )\n",
    "        response = requests.get(url, headers=headers)\n",
    "        return json.dumps(response.json())\n",
    "\n",
    "    tradingview = TradingviewWrapper(llm=llm)\n",
    "\n",
    "    @tool\n",
    "    def search(query: str, search_type: str = None) -> str:\n",
    "        \"\"\"Search for a webpage with Google based on the query.\n",
    "        Set the optional search_type (str) parameter to specify whether to search news (search_type='news') or web pages (search_type=None).\n",
    "        \"\"\"\n",
    "        if search_type == \"news\":\n",
    "            newsSearch = GoogleSerperAPIWrapper(type=search_type, tbs=\"qdr:h\")\n",
    "            return json.dumps(\n",
    "                [\n",
    "                    {\n",
    "                        \"title\": r[\"title\"],\n",
    "                        \"link\": r[\"link\"],\n",
    "                        \"snippet\": r[\"snippet\"],\n",
    "                        \"imageUrl\": r[\"imageUrl\"],\n",
    "                    }\n",
    "                    for r in newsSearch.results(query=query)[\"news\"]\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            searchWebpage = GoogleSerperAPIWrapper()\n",
    "            return json.dumps(\n",
    "                [\n",
    "                    {\n",
    "                        \"title\": r[\"title\"],\n",
    "                        \"link\": r[\"link\"],\n",
    "                        \"snippet\": r[\"snippet\"],\n",
    "                    }\n",
    "                    for r in searchWebpage.results(query=query)[\"organic\"]\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def remove_html_tags(text):\n",
    "        \"\"\"Remove html tags from a string\"\"\"\n",
    "        clean = re.compile(\"<.*?>\")\n",
    "        text = re.sub(clean, \"\", text)  # Remove HTML tags\n",
    "        text = unescape(text)  # Unescape HTML entities\n",
    "        return text\n",
    "\n",
    "    from pyppeteer import launch\n",
    "\n",
    "    async def fetch_page(url):\n",
    "        browser = await launch()\n",
    "        page = await browser.newPage()\n",
    "        await page.goto(url)\n",
    "        html_content = await page.content()\n",
    "        await browser.close()\n",
    "        return html_content\n",
    "\n",
    "    @tool\n",
    "    async def get_contents(links: List[str]):\n",
    "        \"\"\"Get the contents of a webpage.\n",
    "        The links passed in should be a list of links returned from `search`.\n",
    "        \"\"\"\n",
    "        req_tasks = []\n",
    "        results = []\n",
    "        for url in links:\n",
    "            req_tasks.append(fetch_page(url=url))\n",
    "            results.append(\n",
    "                {\n",
    "                    \"url\": url,\n",
    "                }\n",
    "            )\n",
    "        contents = await asyncio.gather(*req_tasks)\n",
    "        extract_task = []\n",
    "        for _content in contents:\n",
    "            no_html = remove_html_tags(_content)\n",
    "            prompt = ChatPromptTemplate.from_template(\n",
    "                \"\"\"I have a piece of text that I need you to help summarize, but please ensure that the summary does not exceed 100 words. Here is the text that needs to be summarized: {input}.\"\"\"\n",
    "            )\n",
    "            model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", verbose=True)\n",
    "            output_parser = StrOutputParser()\n",
    "            chain = prompt | model | output_parser\n",
    "            task = chain.with_config({\"verbose\": True}).ainvoke({\"input\": no_html})\n",
    "            extract_task.append(task)\n",
    "        _extracts = await asyncio.gather(*extract_task)\n",
    "        for i in range(len(results)):\n",
    "            results[i][\"extract\"] = _extracts[i]\n",
    "        return json.dumps(results) if len(results) > 0 else f\"There is no any result\"\n",
    "\n",
    "    # from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "    from arxiv_wrapper import ArxivAPIWrapper\n",
    "\n",
    "    # arxiv = ArxivQueryRun()\n",
    "\n",
    "    @tool\n",
    "    def arxiv_search(query: str):\n",
    "        \"\"\"A wrapper around Arxiv.org\n",
    "        Useful for when you need to answer questions about Physics, Mathematics,\n",
    "        Computer Science, Quantitative Biology, Quantitative Finance, Statistics,\n",
    "        Electrical Engineering, and Economics\n",
    "        from scientific articles on arxiv.org.\n",
    "        Input should be a search query.\"\"\"\n",
    "        api_wrapper = ArxivAPIWrapper(doc_content_chars_max=10000)\n",
    "        return api_wrapper.run(query=query)\n",
    "\n",
    "    @tool\n",
    "    def arxiv_load(entry_id: str):\n",
    "        \"\"\"Useful for when your need to know the content of some paper on Arxiv.org.\n",
    "        Input should be the entry_id return from `arxiv_search`.\"\"\"\n",
    "        api_wrapper = ArxivAPIWrapper(doc_content_chars_max=10000)\n",
    "        return api_wrapper.load(query=entry_id)\n",
    "\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    @tool\n",
    "    def getHTMLFromURL(url: str) -> str:\n",
    "        \"\"\"useful when you need get the HTML of URL. The input to this should be URL.\"\"\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        return soup.prettify()\n",
    "\n",
    "    @tool\n",
    "    async def getHTMLFromURLs(urls: list[str]) -> str:\n",
    "        \"\"\"useful when you need get the HTML of URLs. The input to this should be URL list.\"\"\"\n",
    "        req_tasks = []\n",
    "        for url in urls:\n",
    "            req_tasks.append(fetch_page(url=url))\n",
    "        contents = await asyncio.gather(*req_tasks)\n",
    "        result = \"\"\n",
    "        for c in contents:\n",
    "            soup = BeautifulSoup(c, \"html.parser\")\n",
    "            result += \"\\n\" + remove_html_tags(soup.prettify())\n",
    "        return result\n",
    "\n",
    "    tools = [\n",
    "        search,\n",
    "        getHTMLFromURL,\n",
    "        getHTMLFromURLs,\n",
    "        Tool(\n",
    "            name=\"CryptocurrencyLatestQuote\",\n",
    "            func=cmc_last_quote_api.run,\n",
    "            description=\"\"\"useful when you need get a cryptocurrency's latest quote. The input to this should be a single cryptocurrency's symbol.\"\"\",\n",
    "            coroutine=cmc_last_quote_api.arun,\n",
    "        ),\n",
    "        getTokenMetadata,\n",
    "        Tool(\n",
    "            name=\"BuyOrSellSignal\",\n",
    "            func=tradingview.buySellSignal,\n",
    "            description=\"\"\"Useful when you need to know buy and sell signals for a cryptocurrency. The input to this should be a cryptocurrency's symbol.\"\"\",\n",
    "            coroutine=tradingview.abuySellSignal,\n",
    "        ),\n",
    "        arxiv_search,\n",
    "        arxiv_load,\n",
    "    ]\n",
    "    from zero_scope_agent import zero_scope_tools\n",
    "\n",
    "    tools += zero_scope_tools\n",
    "    return tools\n",
    "\n",
    "\n",
    "def create_agent_executor(llm_agent: Runnable) -> AgentExecutor:\n",
    "\n",
    "    tools=getTools() \n",
    "    \n",
    "    from langchain.tools.render import render_text_description\n",
    "    from langchain_core.runnables import RunnablePassthrough\n",
    "    from langchain.agents.format_scratchpad import format_log_to_str\n",
    "    from langchain.agents.output_parsers import ReActSingleInputOutputParser\n",
    "    from langchain import hub\n",
    "\n",
    "    react_prompt = hub.pull(\"hwchase17/react-chat\")\n",
    "    react_prompt = react_prompt.partial(\n",
    "        tools=render_text_description(list(tools)),\n",
    "        tool_names=\", \".join([t.name for t in tools]),\n",
    "    )\n",
    "    agent = (\n",
    "        # {\n",
    "        #     \"input\": lambda x: x[\"input\"],\n",
    "        #     \"agent_scratchpad\": lambda x: format_log_to_str(\n",
    "        #         x[\"intermediate_steps\"]\n",
    "        #     ),\n",
    "        #     \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        # }\n",
    "        RunnablePassthrough.assign(\n",
    "            agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "        )\n",
    "        | react_prompt\n",
    "        | llm_agent.bind(stop=[\"\\nObservation\"])\n",
    "        | ReActSingleInputOutputParser()\n",
    "    )\n",
    "\n",
    "    executor = AgentExecutor(\n",
    "        agent=agent,\n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "    ).with_config({\"run_name\": \"Eddie's Assistant Agent\"})\n",
    "    return executor\n",
    "\n",
    "\n",
    "from langchain_community.chat_models import ChatPerplexity\n",
    "\n",
    "llm_agent = ChatPerplexity(\n",
    "    model=\"sonar-medium-chat\", temperature=0.9, verbose=True, streaming=True\n",
    ")\n",
    "\n",
    "agent_executor = create_agent_executor(llm_agent=llm_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
